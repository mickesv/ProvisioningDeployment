# Changes from v1-2:
# - Fixing MongoDB to work as a replica set (nothing really to do with Kubernetes, but has to be done)
# - Adding ReadinessProbes to mongodb so that startup waits for mongodb to actually be booted.
#   Not sure this works completely, though. I think the ping answers ok before everything is actually ready.
#
# Changes from v1-1:
# - Added Mongodb database. I am doing this as a separate StatefulSet and a Service, because I want them to run in their own pods.
# - ConfigMap to store hostname for mongodb

# ConfigMap to store environment variables in
# --------------------
# textstore_host MUST be set to NOTSET (case sensitive), since I will use sed to replace this with the right IP number when starting.
# Unfortunately, I now need to start things in stages:
# 1. apply this file and anything with the label "data=config"
# 2. apply this file and anything with the label "app=textstore"
# 3. get the spec.clusterIP from services/textstore-service
# 4. extract the configMap, replace NOTSET with the ipnumber from step 3, and apply as a "new" configmap.
# 5. apply this file and anything with the label "app=qfapp"
#
# Version 1-3: In this version the TEXTSTORE_REPLICASET hardcodes the names of the individual pods that run the mongodb replica set
# This means that I could probably run most of the startup in a single go again.
apiVersion: v1
kind: ConfigMap
metadata:
  name: qfapp-config
  labels:
    data: config
data:
  textstore_host: "NOTSET"
  textstore_replicaset: "mongodb://textstore-0.textstore-service.default.svc.cluster.local:27017,textstore-1.textstore-service.default.svc.cluster.local:27017,textstore-2.textstore-service.default.svc.cluster.local:27017/textStore?replicaSet=MainRepSet"
  textstore_user: "kube"
  textstore_pw: "kubeuser"
  mongo_authsource: "admin" # This is the database where all usera are stored
  mongo_user: "main_admin"
  mongo_pw: "hunter2"
---

# QFStandalone
# --------------------
#
# Service (LoadBalancer) to expose qfstandalone
# "create a service before its corresponding backend..." https://kubernetes.io/docs/concepts/configuration/overview/
#
# Using "sessionAffinity=ClientIP" means that I can have more than one replica of the qfstandalone deployment,
# since routing from the same client is always done to the same pod. This takes care of the socket-talk between
# each client and their (now) corresponding express-server.
# 
apiVersion: v1
kind: Service
metadata:
  name: qfapp-service
  labels:
    app: qfapp
spec:
  type: LoadBalancer
  sessionAffinity: ClientIP
  ports:
  - name: http
    port: 3000
    nodePort: 30001
    protocol: TCP
  selector:
    app: qfapp
---
# Deployment to launch one container of mickesv/qfstandalone in a pod.
# A bit of repetitive "label app: qfapp" everywhere, but I /think/ they are all needed.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qfstandalone
  labels:
    app: qfapp
    startup: qfapp-wait
spec:
  replicas: 1
  selector:
    matchLabels:
      app: qfapp
  template:
    metadata:
      labels:
        app: qfapp
    spec:
      containers:
      - name: qfstandalone
        image: mickesv/qfstandalone:version1
        imagePullPolicy: Always
        command: ["nodemon", "-L", "-P", "1000", "src/index.js"] # Superimposing the default start command in order to add the "-L" legacy-watch flag
        volumeMounts:
        - mountPath: /app/src
          name: source-dir
        ports:
        - containerPort: 3000
        env:
        - name: TEXTSTORE_HOST
          valueFrom:
            configMapKeyRef: 
              name: qfapp-config
              key: textstore_host
        - name: TEXTSTORE_RS
          valueFrom:
            configMapKeyRef:
              name: qfapp-config
              key: textstore_replicaset
        - name: TEXTSTORE_USER
          valueFrom:
            configMapKeyRef:
              name: qfapp-config
              key: textstore_user
        - name: TEXTSTORE_PW
          valueFrom:
            configMapKeyRef:
              name: qfapp-config
              key: textstore_pw
        - name: MONGO_AUTHSOURCE
          valueFrom:
            configMapKeyRef:
              name: qfapp-config
              key: mongo_authsource
      volumes:
      - name: source-dir
        hostPath:
          path: /app/src
          type: Directory
---
# TextStore
# --------------------
# - one container per pod running the image mongodb
# - one headless service to access them.
# - Two persistentVolumeClaims/mounts: textstore-data, and mongo-config
# 
# I also need two PersistentVolumes to match these claims. As per recommendations
# these are specified in a separate file (so that someone with the right privileges can launch them)
# Scrath that: the PV's are created automatically by minikube. May be different once you start
# deploying to SomewhereOnTheCloud.
# 
# There are two paths forward here:
# - One is to create separate resources for each PersistentVolumeClaim and then refer to these using the ...spec.volumes
#   This will not work, since all replicas will then bind to this same PersistentVolume (claim), and MongoDB will choke
#   since there is already a lock file in place.
#   I am keeping this config commented out below the working config , for future reference.
#
# - The other is to use volumeClaimTemplates, that specifies what /each/ replica in the StatefulSet desires.
#
#   WARNING: If you then specify spec.storageClassName, kubernetes will go looking for that particular storageClass.
#   If you /have/ this class specified, kubiot will dutifully wait for you to attach a persistentVolume to match that class.
#   If you do attach a PV to the StorageClass, you are almost back to the first problem; for the next replica
#   kubernetes will wait for /another/ PV to be attached to the StorageClass.
#   ... So, the only solution I have right now is:
#    - Do not specify spec.storageClassName
#    - Do not specify any StorageClasses
#    - Do not specify any PersistentVolumes
#
#    :grumpy-cat: Me no like.
#
---
apiVersion: v1
kind: Service
metadata:
  name: textstore-service
  labels:
    app: textstore
spec:
  clusterIP: 10.96.0.101   # Hardcoded clusterIP, but I don't think I need this anymore (with the configMap shenanigans)
  ports:
  - port: 27017
    targetPort: mongodb-port
  selector:
    app: textstore
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: textstore
  labels:
    app: textstore
spec:
  serviceName: textstore-service  # This becomes part of the hostname (e.g. textstore-0.textstore-service.default.svc.cluster.local)
  replicas: 3
  selector:
    matchLabels:
      app: textstore
  template:
    metadata:
      labels:
        app: textstore
        startup: textstore-wait
    spec:
      containers:
      - name: textstore
        image: mongo
        command: ["mongod", "--bind_ip", "0.0.0.0", "--replSet", "MainRepSet", "--auth", "--clusterAuthMode", "keyFile", "--keyFile", "/etc/secrets-volume/mongodb-keyfile", "--setParameter", "authenticationMechanisms=SCRAM-SHA-1"]
        readinessProbe:
          exec:
            command: ["mongosh", "--eval", "db.adminCommand({ping: 1})"]
        ports:
        - containerPort: 27017
          name: mongodb-port
        volumeMounts:
        - name: textstore-data
          mountPath: /data/db
        - name: mongo-config
          mountPath: /data/configdb
        - name: secrets-volume
          readOnly: true
          mountPath: /etc/secrets-volume
      volumes:
      - name: secrets-volume
        secret:
          secretName: mongodb-bootstrap-data
          defaultMode: 256
  volumeClaimTemplates:
    - metadata:
        name: textstore-data
      spec:
        accessModes: ["ReadWriteOnce"]
#        storageClassName: data
        resources:
          requests: 
            storage: 10Mi
    - metadata:
        name: mongo-config
      spec:
        accessModes: ["ReadWriteOnce"]
#        storageClassName: config
        resources:
          requests: 
            storage: 5Mi          
---

# --------------------
# This attempt does not work since all replicas bind to the same PersistentVolume
# and MongoDB does not like this.
# --------------------
# ---
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: textstore-data
#   labels:
#     app: textstore
# spec:
#   accessModes:
#   - ReadWriteOnce
#   storageClassName: data
#   resources:
#     requests:
#       storage: 10Mi
# ---
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: textstore-config
#   labels:
#     app: textstore
# spec:
#   accessModes:
#   - ReadWriteOnce
#   storageClassName: config
#   resources:
#     requests:
#       storage: 5Mi
# ---
# apiVersion: apps/v1
# kind: StatefulSet
# metadata:
#   name: textstore
#   labels:
#     app: textstore
# spec:
#   serviceName: mongodb
#   replicas: 2
#   selector:
#     matchLabels:
#       app: textstore
#   template:
#     metadata:
#       labels:
#         app: textstore
#     spec:
#       containers:
#       - name: textstore
#         image: mongo
#         ports:
#         - containerPort: 27017
#           name: mongodb-port
#         volumeMounts:
#         - name: textstore-data
#           mountPath: /data/db
#         - name: mongo-config
#           mountPath: /data/configdb
#       volumes:
#         - name: textstore-data
#           persistentVolumeClaim: 
#             claimName: textstore-data
#         - name: mongo-config
#           persistentVolumeClaim: 
#             claimName: textstore-config
# ---
